@misc{Buchfink2014,
abstract = {The alignment of sequencing reads against a protein reference database is a major computational bottleneck in metagenomics and data-intensive evolutionary projects. Although recent tools offer improved performance over the gold standard BLASTX, they exhibit only a modest speedup or low sensitivity. We introduce DIAMOND, an open-source algorithm based on double indexing that is 20,000 times faster than BLASTX on short reads and has a similar degree of sensitivity.},
author = {Buchfink, Benjamin and Xie, Chao and Huson, Daniel H.},
booktitle = {Nature Methods},
doi = {10.1038/nmeth.3176},
issn = {15487105},
title = {{Fast and sensitive protein alignment using DIAMOND}},
year = {2014}
}
@article{Buneman2001,
author = {Buneman, Peter and Khanna, Sanjeev and Tan, Wang-Chiew and Chiew, Wang -},
journal = {Lecture Notes in Computer Science},
number = {January},
pages = {316--330},
title = {{Why and Where: A Characterization of Data Provenance}},
url = {http://homepages.inf.ed.ac.uk/opb/papers/ICDT2001.pdf},
volume = {1973},
year = {2001}
}
@article{camacho2009blast,
author = {Camacho, Christiam and Coulouris, George and Avagyan, Vahram and Ma, Ning and Papadopoulos, Jason and Bealer, Kevin and Madden, Thomas L},
journal = {BMC bioinformatics},
number = {1},
pages = {421},
publisher = {BioMed Central},
title = {{BLAST+: architecture and applications}},
volume = {10},
year = {2009}
}
@article{Cock2009,
author = {Cock, Peter J.A. and Antao, Tiago and Chang, Jeffrey T. and Chapman, Brad A. and Cox, Cymon J. and Dalke, Andrew and Friedberg, Iddo and Hamelryck, Thomas and Kauff, Frank and Wilczynski, Bartek and {De Hoon}, Michiel J.L.},
doi = {10.1093/bioinformatics/btp163},
issn = {13674803},
journal = {Bioinformatics},
title = {{Biopython: Freely available Python tools for computational molecular biology and bioinformatics}},
year = {2009}
}
@article{DePaula2013,
abstract = {In this work, we used the PROV-DM model to manage data provenance in workflows of genome projects. This provenance model allows the storage of details of one workflow execution, e.g., raw and produced data and computational tools, their versions and parameters. Using this model, biologists can access details of one particular execution of a workflow, compare results produced by different executions, and plan new experiments more efficiently. In addition to this, a provenance simulator was created, which facilitates the inclusion of provenance data of one genome project workflow execution. Finally, we discuss one case study, which aims to identify genes involved in specific metabolic pathways of Bacillus cereus, as well as to compare this isolate with other phylogenetic related bacteria from the Bacillus group. B. cereus is an extremophilic bacteria, collected in warm water in the Midwestern Region of Brazil, its DNA samples having been sequenced with an NGS machine.},
author = {de Paula, Renato and Holanda, Maristela and Gomes, Luciana S.A. and Lifschitz, Sergio and Walter, Maria Emilia M.T.},
doi = {10.1186/1471-2105-14-S11-S6},
issn = {14712105},
journal = {BMC bioinformatics},
number = {Suppl 11},
pages = {S6},
publisher = {BioMed Central},
title = {{Provenance in bioinformatics workflows.}},
volume = {14 Suppl 1},
year = {2013}
}
@article{DiTommaso2017,
abstract = {In spite of over two decades of intense research, illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications. The objective of this work is to recognize faces using video sequences both for training and recognition input, in a realistic, unconstrained setup in which lighting, pose and user motion pattern have a wide variability and face images are of low resolution. The central contribution is an illumination invariant, which we show to be suitable for recognition from video of loosely constrained head motion. In particular there are three contributions: (i) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation to exploit the proposed invariant and generalize in the presence of extreme illumination changes; (ii) we introduce a video sequence re-illumination algorithm to achieve fine alignment of two video sequences; and (iii) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve robustness to unseen head poses. We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 323 individuals and 1474 video sequences with extreme illumination, pose and head motion variation. Our system consistently achieved a nearly perfect recognition rate (over 99.7{\%} on all four databases). ?? 2012 Elsevier Ltd All rights reserved.},
author = {{Di Tommaso}, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
doi = {10.1038/nbt.3820},
issn = {15461696},
journal = {Nature Biotechnology},
number = {4},
pages = {316--319},
title = {{Nextflow enables reproducible computational workflows}},
volume = {35},
year = {2017}
}

@misc{Dong2020,
author = {Dong, Trung},
journal = {GitHub repository},
publisher = {GitHub},
title = {{PROV: A Python library for W3C Provenance Data Model}},
year = {2020}
}
@article{Edgar2004,
abstract = {We describe MUSCLE, a new computer program for creating multiple alignments of protein sequences. Elements of the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy of MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets of reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each of these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest of the tested methods for large numbers of sequences, aligning 5000 sequences of average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5.com/muscle. {\textcopyright} Oxford University Press 20004; all rights reserved.},
author = {Edgar, Robert C.},
doi = {10.1093/nar/gkh340},
issn = {03051048},
journal = {Nucleic Acids Research},
number = {5},
pages = {1792--1797},
pmid = {15034147},
title = {{MUSCLE: Multiple sequence alignment with high accuracy and high throughput}},
volume = {32},
year = {2004}
}
@book{Groth2013,
author = {Groth, Paul and Moreau, Luc},
doi = {10.2200/S00528ED1V01Y201308WEB007},
editor = {Hendler, James and Ding, Ying},
isbn = {9781627052214},
pages = {131},
publisher = {Morgan {\&} Claypool},
title = {{An Introduction to PROV}},
year = {2013}
}
@article{Hyatt2010,
abstract = {BACKGROUND: The quality of automated gene prediction in microbial organisms has improved steadily over the past decade, but there is still room for improvement. Increasing the number of correct identifications, both of genes and of the translation initiation sites for each gene, and reducing the overall number of false positives, are all desirable goals.$\backslash$n$\backslash$nRESULTS: With our years of experience in manually curating genomes for the Joint Genome Institute, we developed a new gene prediction algorithm called Prodigal (PROkaryotic DYnamic programming Gene-finding ALgorithm). With Prodigal, we focused specifically on the three goals of improved gene structure prediction, improved translation initiation site recognition, and reduced false positives. We compared the results of Prodigal to existing gene-finding methods to demonstrate that it met each of these objectives.$\backslash$n$\backslash$nCONCLUSION: We built a fast, lightweight, open source gene prediction program called Prodigal http://compbio.ornl.gov/prodigal/. Prodigal achieved good results compared to existing methods, and we believe it will be a valuable asset to automated microbial annotation pipelines.},
author = {Hyatt, Doug and Chen, Gwo-Liang and Locascio, Philip F and Land, Miriam L and Larimer, Frank W and Hauser, Loren J},
doi = {10.1186/1471-2105-11-119},
isbn = {1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Bacterial,Databases,Genetic,Genome,Peptide Chain Initiation,Prokaryotic Cells,Software,Translational,Translational: genetics},
pages = {119},
pmid = {20211023},
title = {{Prodigal: prokaryotic gene recognition and translation initiation site identification.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2848648{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {11},
year = {2010}
}
@article{Koster2012,
abstract = {Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames. {\textcopyright} The Author 2012. Published by Oxford University Press. All rights reserved.},
author = {K{\"{o}}ster, Johannes and Rahmann, Sven},
doi = {10.1093/bioinformatics/bts480},
issn = {14602059},
journal = {Bioinformatics},
number = {19},
pages = {2520--2522},
pmid = {29788404},
title = {{Snakemake-a scalable bioinformatics workflow engine}},
volume = {28},
year = {2012}
}
@article{Lakin2017,
abstract = {{\textcopyright} The Author(s) 2016. Antimicrobial resistance has become an imminent concern for public health. As methods for detection and characterization of antimicrobial resistance move from targeted culture and polymerase chain reaction to high throughput metagenomics, appropriate resources for the analysis of large-scale data are required. Currently, antimicrobial resistance databases are tailored to smaller-scale, functional profiling of genes using highly descriptive annotations. Such characteristics do not facilitate the analysis of large-scale, ecological sequence datasets such as those produced with the use of metagenomics for surveillance. In order to overcome these limitations, we present MEGARes (https://megares.meglab.org), a hand-curated antimicrobial resistance database and annotation structure that provides a foundation for the development of high throughput acyclical classifiers and hierarchical statistical analysis of big data. MEGARes can be browsed as a stand-alone resource through the website or can be easily integrated into sequence analysis pipelines through download. Also via the website, we provide documentation for AmrPlusPlus, a user-friendly Galaxy pipeline for the analysis of high throughput sequencing data that is pre-packaged for use with the MEGARes database.},
author = {Lakin, Steven M. and Dean, Chris and Noyes, Noelle R. and Dettenwanger, Adam and Ross, Anne Spencer and Doster, Enrique and Rovira, Pablo and Abdo, Zaid and Jones, Kenneth L. and Ruiz, Jaime and Belk, Keith E. and Morley, Paul S. and Boucher, Christina},
doi = {10.1093/nar/gkw1009},
issn = {13624962},
journal = {Nucleic Acids Research},
month = {jan},
number = {D1},
pages = {D574--D580},
publisher = {Oxford University Press},
title = {{MEGARes: An antimicrobial resistance database for high throughput sequencing}},
volume = {45},
year = {2017}
}
@article{Markowetz2017,
author = {Markowetz, Florian},
doi = {10.1371/journal.pbio.2002050},
issn = {15457885},
journal = {PLoS Biology},
number = {3},
pages = {4--7},
pmid = {28278152},
title = {{All biology is computational biology}},
volume = {15},
year = {2017}
}
@article{Mattoso2010,
author = {Mattoso, Marta and Werner, Claudia and Travassos, Guilherme Horta and Braganholo, Vanessa and Ogasawara, Eduardo and Oliveira, Daniel De and Cruz, Sergio Manuel Serra Da and Martinho, Wallace and Murta, Leonardo},
doi = {10.1504/ijbpim.2010.033176},
issn = {1741-8763},
journal = {International Journal of Business Process Integration and Management},
number = {1},
pages = {79},
title = {{Towards supporting the life cycle of large scale scientific experiments}},
volume = {5},
year = {2010}
}
@article{Kanwal2017,
abstract = {Background: Computational bioinformatics workflows are extensively used to analyse genomics data, with different approaches available to support implementation and execution of these workflows. Reproducibility is one of the core principles for any scientific workflow and remains a challenge, which is not fully addressed. This is due to incomplete understanding of reproducibility requirements and assumptions of workflow definition approaches. Provenance information should be tracked and used to capture all these requirements supporting reusability of existing workflows. Results: We have implemented a complex but widely deployed bioinformatics workflow using three representative approaches to workflow definition and execution. Through implementation, we identified assumptions implicit in these approaches that ultimately produce insufficient documentation of workflow requirements resulting in failed execution of the workflow. This study proposes a set of recommendations that aims to mitigate these assumptions and guides the scientific community to accomplish reproducible science, hence addressing reproducibility crisis. Conclusions: Reproducing, adapting or even repeating a bioinformatics workflow in any environment requires substantial technical knowledge of the workflow execution environment, resolving analysis assumptions and rigorous compliance with reproducibility requirements. Towards these goals, we propose conclusive recommendations that along with an explicit declaration of workflow specification would result in enhanced reproducibility of computational genomic analyses.},
author = {Kanwal, Sehrish and Khan, Farah Zaib and Lonie, Andrew and Sinnott, Richard O.},
doi = {10.1186/s12859-017-1747-0},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Common Workflow Language (CWL),Cpipe,Galaxy,Provenance,Reproducibility,Workflow},
month = {jul},
number = {1},
pages = {337},
publisher = {BioMed Central Ltd.},
title = {{Investigating reproducibility and tracking provenance - A genomic workflow case study}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1747-0},
volume = {18},
year = {2017}
}
@article{Katoh2005,
abstract = {The accuracy of multiple sequence alignment proram MAFFT has been improved. The new version (5.3) of MAFFT offers new iterative refinement options, H-INS-i, F-INS-i and G-INS-i, in which pairwise alignment information are incorporated into objective function. These new options of MAFFT showed higher accuracy than currently available methods including TCoffee version 2 and CLUSTAL W in benchmark tests consisting of alignments of {\textgreater}50 sequences. Like the previously available options, the new options of MAFFT can handle hundreds of sequences on a standard desktop computer. We also examined the effect of the number of homologues included in an alignment. For a multiple alignment consisting of ∼8 sequences with low similarity, the accuracy was improved (2-10 percentage points) when the sequences were aligned together with dozens of their close homologues (E-value {\textless} 10-5-10-20) collected from a database. Such improvement was generally observed for most methods, but remarkably large for the new options of MAFFT proposed here. Thus, we made a Ruby script, mafftE.rb, which aligns the input sequences together with their close homologues collected from SwissProt using NCBI-BLAST. {\textcopyright} Oxford University Press 2005; all rights reserved.},
author = {Katoh, Kazutaka and Kuma, Kei Ichi and Toh, Hiroyuki and Miyata, Takashi},
doi = {10.1093/nar/gki198},
issn = {03051048},
journal = {Nucleic Acids Research},
number = {2},
pages = {511--518},
pmid = {15661851},
title = {{MAFFT version 5: Improvement in accuracy of multiple sequence alignment}},
volume = {33},
year = {2005}
}
@article{Khan2019,
abstract = {Background: The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Computationally driven data-intensive experiments using workflows enable automation, scaling, adaptation, and provenance support. However, there are still several challenges associated with the effective sharing, publication, and reproducibility of such workflows due to the incomplete capture of provenance and lack of interoperability between different technical (software) platforms. Results: Based on best-practice recommendations identified from the literature on workflow design, sharing, and publishing, we define a hierarchical provenance framework to achieve uniformity in provenance and support comprehensive and fully re-executable workflows equipped with domain-specific information. To realize this framework, we present CWLProv, a standard-based format to represent any workflow-based computational analysis to produce workflow output artefacts that satisfy the various levels of provenance. We use open source community-driven standards, interoperable workflow definitions in Common Workflow Language (CWL), structured provenance representation using the W3C PROV model, and resource aggregation and sharing as workflow-centric research objects generated along with the final outputs of a given workflow enactment. We demonstrate the utility of this approach through a practical implementation of CWLProv and evaluation using real-life genomic workflows developed by independent groups. Conclusions: The underlying principles of the standards utilized by CWLProv enable semantically rich and executable research objects that capture computational workflows with retrospective provenance such that any platform supporting CWL will be able to understand the analysis, reuse the methods for partial reruns, or reproduce the analysis to validate the published findings.},
author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O. and Lonie, Andrew and Goble, Carole and Crusoe, Michael R.},
doi = {10.1093/gigascience/giz095},
issn = {2047217X},
journal = {GigaScience},
keywords = {BagIt,CWL,Common Workflow Language,RO,Research Object,containers,interoperability,provenance,scientific workflows},
number = {11},
pmid = {31675414},
title = {{Sharing interoperable workflow provenance: A review of best practices and their practical application in CWLProv}},
volume = {8},
year = {2019}
}

@article{Pasquier2017,
author = {Pasquier, Thomas and Lau, Matthew K. and Trisovic, Ana and Boose, Emery R. and Couturier, Ben and Crosas, Merc{\`{e}} and Ellison, Aaron M. and Gibson, Valerie and Jones, Chris R. and Seltzer, Margo},
doi = {10.1038/sdata.2017.114},
issn = {20524463},
journal = {Scientific Data},
pages = {1--5},
title = {{If these data could talk}},
volume = {4},
year = {2017}
}
@inproceedings{ragan2014jupyter,
author = {Ragan-Kelley, Min and Perez, F and Granger, B and Kluyver, T and Ivanov, P and Frederic, J and Bussonnier, M},
booktitle = {AGU Fall Meeting Abstracts},
title = {{The Jupyter/IPython architecture: a unified view of computational research, from interactive exploration to communication and publication.}},
year = {2014}
}

@article{Silva2018,
abstract = {We present DfAnalyzer, a tool that enables monitoring, debugging, steering, and analysis of dataflows while being generated by scientific applications. It works by capturing strategic domain data, registering provenance and execution data to enable queries at runtime. DfAnalyzer provides lightweight dataflow monitoring components to be invoked by high performance applications. It can be plugged in scientific code scripts, or Spark applications, in the same way users already plug visualization library components. During this demo, we will show how DfAnalyzer captures the dataflow, provenance, as well as how it provides runtime data analyses of applications. We will also encourage attendees to use DfAnalyzer for their own applications.},
author = {Silva, V{\'{i}}tor and de Oliveira, Daniel and Valduriez, Patrick and Mattoso, Marta},
doi = {10.14778/3229863.3236265},
number = {12},
pages = {2082--2085},
title = {{DfAnalyzer: Runtime Dataflow Analysis of Scientific Applications using Provenance}},
url = {https://doi.org/10.14778/3229863.3236265},
volume = {11},
year = {2018}
}
@article{Stevens2007,
author = {Stevens, Robert and Zhao, Jun and Goble, Carole},
doi = {10.1093/bib/bbm015},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Data derivation,In Silico experiments,Provenance,Validation and verification of results,Workflow},
number = {3},
pages = {183--194},
title = {{Using provenance to manage knowledge of In Silico experiments}},
volume = {8},
year = {2007}
}
@article{wilson2017good,
author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K},
journal = {PLOS Computational Biology},
number = {6},
doi = {10.1371/journal.pcbi.1005510},
pages = {e1005510},
publisher = {Public Library of Science},
title = {{Good enough practices in scientific computing}},
volume = {13},
year = {2017}
}
